---
layout:     post
title:      "Tensorflow 笔记（八）"
subtitle:   "卷积神经网络基础"
date:       2019-04-19 00:15:18
author:     "Pelhans"
header-img: "img/tensorflow.jpg"
header-mask: 0.3 
catalog:    true
tags:
    - Deep Learning
---

> 眼睛看过只是别人的，整理出来并反复学习才是自己的。

* TOC
{:toc}

# 概览
卷积神经网络 是一种专门用来处理具有类似网格结构的数据的神经网络。CNN 近年来在很多领域都表现优异。卷积神经网络依次的来源是因为该网络使用了卷积这种数学运算。卷积是一种特殊的线性运算。卷积网络是指那些至少在网络的一层中使用卷积运算来替代一般的矩阵乘法运算的神经网络。

卷积网络中一个典型层包含三级。第一级是卷积级，并行地计算多个卷积产生一组线性激活响应。第二级是探测级，每一个线性激活响应都会通过一个非线性的激活函数，如 ReLU。第三极叫池化级，这里通过池化函数来进一步调整输出。更高级的卷积神经网络是这些基本组件的深度结合。

# 什么是卷积

摘自[百度百科](https://baike.baidu.com/item/%E5%8D%B7%E7%A7%AF/9411006?fr=aladdin)：在泛函分析中，卷积是通过两个函数 x 和w 生成第三个函数的一种数学算子，表征函数x与w经过旋转和平移的重叠部分的面积。如果将参加卷积的一个函数看做区间的指示函数，卷积还可以被看做是滑动平均的推广。从公式上来看，假设 f(a)、g(a)是$R^{1}$上的两个可积函数，做积分：

$$ h(t) = \int_{-\infty}^{\infty} x(a)w(t-a)da $$

h(t)就称为 函数 x 与 w 的卷积，记为 $h(x) = (x*w)(t) $$

在卷积神经网络术语中，我们把 x 叫做输入，w 叫做核函数，输出有时叫特征映射。当然，在实际的网络中，输入往往是离散化的，因此上述积分可以转化为请求和的形式。因为输入的范围也是有限的，因此上式可以进一步的简化。比如如果把一张二维的图像I 作为输入，用二维卷核 K 做卷积，则：

$$ S(i,j) = (I*K)(i,j) = \sum_{m}\sum_{n}I(m,n)K(i-m, j-n) $$

卷积是可交换的，因此上式还可以写作：

$$ S(i, j) = (K*I)(i,j) = \sum_{m}\sum_{n}I(i-m, j-n)K(m,n) $$

原书中没说 m,n,i,j代表什么意思。我猜测 m,n 是卷积核的索引，i,j 是输入的索引。下图给出一个在二维张量上的卷积运算的例子：

![](/img/in-post/tensorflow/cnn_jisuan.png)    

# 为什么要用卷积

卷积运算通过三个重要的思想来帮助改进机器学习系统：稀疏权重、参数共享、等变表示。

## 稀疏权重

传统的矩阵运算，如果有 m 个输入和 n 个输出，那么需要 m*n 个参数。而采用卷积运算后，由于卷积核的大小一般都不大，如为k，那么此时的输出需要 k*n个参数就够了。下图可以解释这个：

![](/img/in-post/tensorflow/cnn_xishu_1.png)

除了权重稀疏了之外，从另一个角度看，在卷积网络中，尽管直接连接都是很稀疏的，但处在更深层次的单元可以间接地连接到全部或者大部分输入图像。

![](/img/in-post/tensorflow/cnn_xishu_2.png)

## 参数共享

参数共享是指在一个模型的多个函数中使用相同的参数。在传统的神经网络中，当计算一层的输出时，权重矩阵的元素只使用一次。而卷积运算的**参数共享是说核的每一个元素都作用在输入的每一个位置上**(说白了就是一个核扫遍整个输入,而不是动一下一个核)。卷积运算中的参数共享保证了我们只需要学习一个参数集合，而不是对于每一个位置都需要学习一个单独的参数集合。

## 等变表示
对于卷积，参数共享的特殊形式使得神经网络层具有对平移等变的性质。如果一个函数满足输入改变，输出也以同样的形式改变这一性质，我们就说它是等变的。特别低，如果函数 f(x) 与 g(x) 满足 f(g(x)) = g(f(x)) ，我们就说 f(x) 对与变换 g 具有等变性。

对于卷积来说，如果零 g 是输入的任意平移函数，那么卷积函数对于 g 具有等变性。也就是说，通过函数g 把输入I平移后进行卷积得到的结果，与先对I进行卷积再进行平移得到的结果是一样的。

# 卷积之外的组件-池化
池化函数使用某一位置的相邻输出的总体统计特征来代替网络在该位置的输出，常用的池化函数包含 最大池化、平均池化、L2范数以及基于距中心像素距离的加权平均函数等。

经过池化函数后的大多数输出并不会发生改变，具有平移不变性。局部平移不变性是一个很重要的性质，尤其是当我们关心某个特征是否出现而不关心它出现的具体位置的时候。使用池化可以看做增加了一个无限强的先验：这一层学得的函数必须具有对少量平移的不变性，当这个假设成立时，池化可以级大地提高网络的统计效率。当然，对空间区域进行池化产生了平移不变性，但当我们对分离参数(离散参数？)的卷积进行池化时。特征能够学得应该对于哪种变换具有不变性(就是说你对原始输入直接上池化能学习一定的平移不变性，要是输入经过卷积和非线性后在来池化，应该能学到其他的不变性)。从贝叶斯的角度看，可以把卷积网络当做一个具有无限强先验的全连接网络。其中卷积的使用当做对网络中一层的参数引入了一个无限强的先验概率分布，这个先验说明了该层应该学得的函数只包含局部连接关系并且对平移具有等变性。类似地，池化相当于每一个单元都具有对少量平移不变性的无限强先验(无限强先验需要对一些参数的概率置零并且完全禁止对这些参数赋值)。

除了可以学习不变性之外，池化还有其他的优点和用处。如因为池化综合了全部邻居的反馈，因此池化单元可以用于降采样。另外，对于很多任务来说，如关系抽取的 multi-instance 中，输入的维度是不定的，但经过最大池化后，输出可以达到统一。

# CNN 的反向传播
CNN 的前向传播比较容易理解，那么它的参数怎么更新呢？拿来一个实例比较容易理解但是不具有通用性。而且最后的卷积核旋转180度是什么鬼？怎么你把梯度填充零就正好了？这里有一个很棒的博客[Convolutional Neural Networks backpropagation: from intuition to derivation](https://grzegorzgwardys.wordpress.com/2016/04/22/8/)。我这里按照我的理解整理出来，方便自己记忆。

我们知道，卷积操作和正常全连接的不同在于接收的输入有限并且参数共享。因此反向传播比较麻烦，但这位小哥将卷积网络转化为减少连接数和权值共享的MLP：

![](/img/in-post/tensorflow/mlp-convolution-transform.png)


