---
layout:     post
title:      "深度学习笔记（零）"
subtitle:   "常见任务的评价指标"
date:       2019-04-08 00:15:18
author:     "Pelhans"
header-img: "img/background.jpg"
header-mask: 0.3 
catalog:    true
tags:
    - Deep Learning
---

* TOC
{:toc}

# 概览

本文总结以下评价指标：

* Acc    
* F1、精准度 P、召回率 R，P@10、R@10    
* AUC、ROC    
* NDCG@3    
* MAP、MRR

首先给出一个表格：

![](/img/in-post/ml_mianshi/metric_table.png)

其中上面行表示预测结果，列表示 ground truth。则

# 准确率 Acc
根据上面的表格，则准确率 Acc 的计算公式为：

$$ \frac{TP + TN}{TP+FN + FP + TN} $$

表示所有预测结果中正确预测的比例。正例的预测和负例的预测 都计入准确率。

# 精准度 Precision

精准度的计算公式为：

$$ \frac{TP}{TP  + FP} $$

表示所有预测为正例的事件中，真正为正例的比率。个人理解就是，它相当于是你说一个事件为正例的可信程度。

## P@N

在信息检索那看到的指标，可以理解为返回 top N 结果的精准度。

# 召回率 Recall

召回率的计算公式为：

$$ \frac{TP}{TP + FN} $$

表示对于所有 ground truth 为 正 P 的事件，你找到（预测）为正的比例。比如河里有 100 条红鲤鱼（正例）和100 个白鲤鱼（负例）。你一个大网下去，捞上来 80 只红鲤鱼，那召回率就是 0.8。

## R@N

topN 结果的召回率

# F score

F 值是对精确率和召回率赋不同权重进行加权调和：

$$ F_{\alpha} = \frac{1 + \alpha^{2}*P*R}{\alpha^{2}*P + R} $$

当 $$ \alpha$$ 为 1 时，就是我们熟知的 F1 值了。

$$ F1 = \frac{2P*R}{P+R} $$

F1 是很常用的评价指标，可以很好的平衡精准率和召回率，且调和平均后，可以很好的照顾到分值较低的指标，可以较为客观的评价模型。

## Macro-F1 和 Micro-F1

* Macro averaged F-score: 根据每个类型的值来计算,得到平均值, 相当于把每个类型平等对待    
* Micro averaged F-score: 综合所有实体的所有类别的贡献来计算平均值, 相当于把每个实体平等看待

个人一般用 Macro F1。

# ROC (Receiver Operating Characteristic Curve)

ROC 曲线是以假正率为横坐标，真正率为纵坐标绘制的曲线，其中：

$$ 真正率 = \frac{TP}{TP + FN} $$

$$ 假正率 = \frac{FP}{FP + TN} $$

对于模型某一阈值，我们可以计算得到一个 真正率和假正率。通过阈值的不断变化，我们就可以得到一条曲线，如下图所示：

![](/img/in-post/ml_mianshi/roc.jpg)

## AUC (Area under the Curve of ROC)

AUC 就是 ROC 曲线下方的面积，AUC等于随机挑选一个正样本（P）和负样本（N）时，分类器将正样本排前面的概率。一般在 0.5 - 1.0 之间。

* 当AUC = 1时，是完美分类器，采用这个预测模型时，存在至少一个阈值能得出完美预测。    
* 当AUC = 0.5时，跟随机猜测一样，模型没有预测价值。

一般可以用AUC评价模型能力，选取好的模型之后根据实际需求确定阈值，再用Macro F1计算性能指标。

# NDCG@N

NDCG可以拆解为四个部分，分别是N（Normalization）标准化、D（Discounted）折减、C（Cumulative）累积、G（Gain）增益。四个部分通过下式表示NDCG：

$$ NDCG@N(x) = \frac{1}{N}\sum_{i=1}^{N}\frac{G}{D} $$

* x表示一个查询，n表示用返回的前n个答案计算本次查询的NDCG，i表示第几个答案    
* G可以理解为一个返回的答案对于本次查询质量的加分。G的大小与i无关，只取决于这个答案的质量，可以用相关度作为该项分值，也可以用 $$2^{相关度}$$这种形式的    
* D可以理解为对于一个加分的适当减分。因为越靠前的答案应该加分越多，越靠后的答案加分越少，加分G是与答案的位置前后无关的，所以需要通过D来控制加分大小。所以D是一个随答案位置i增大而增大的量，比如用$$log(i)$$    
* C是对1到n个位置的G/D进行累加，得到这次查询的质量得分    
* N是对得分进行归一化处理，可以理解为N是理想情况下的得分，即能够取得的最高得分    

# MAP

MAP 是 AP的平均值，在信息检索中，AP指的是不同召回率上的正确率的平均值，而现在的有些检索系统为了能够快速返回结果，在计算AP时就不再考虑召回率。换句话说，如果数据库中和查询信息相关的5条信息，分别出现在查询结果中的第1、3、6、9、10位，那么这次查询的AP就是： 

$$ AP = (1/1 + 2/3 + 3/6 + 4/9 + 5/10)/5 = 0.62 $$ 

多次查询就可以得到多个 AP ，对其取平均即可得到 MAP。

# MRR(Mean Reciprocal Rank)

把标准答案在被评价系统给出结果中的排序取倒数作为它的准确度，再对所有的问题取平均。如query1 中标准答案被排在第3 位，query 2 中的排在第2 位，query 3 的排在第一位，则 MRR 为：

$$ (1/3 + 1/2 + 1/1)/3 = 0.61 $$
