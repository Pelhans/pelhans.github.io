---
layout:     post
title:      "Tensorflow 笔记（七）"
subtitle:   "激活函数"
date:       2019-04-17 00:15:18
author:     "Pelhans"
header-img: "img/tensorflow.jpg"
header-mask: 0.3 
catalog:    true
tags:
    - Deep Learning
---

> 眼睛看过只是别人的，整理出来并反复学习才是自己的。

* TOC
{:toc}

# 概览

激活函数是神经网络的一个重要组成部分，它可以将线性分类器转换为非线性分类器，这已被证明是近年来在各种任务重所见到的高性能的关键。不同的激活函数在实践中经常表现出非常多样的行为。例如 Sigmoid 函数，在过去非常流行。它可以将任意范围的输入转化为0-1之间的输出，逻辑回归中就用它来做二分类问题。在早起网络不深的时候用它效果很好，但随着网络的加深，函数两端饱和区域的劣势就显现出来，很容易导致梯度的消失。 tanh 函数相比于 sigmoid 函数，它可以将输出映射到-1 - 1内，激活函数可以为负值，网络表达能力可能会有提升。再后来有了ReLU 激活函数，它的形式非常简单，相比于前两个，它的导数在正值区恒定，梯度不易消失，同事它还相当于一个滤波器，可以降低网络信息冗余度。目前 RuLU 依旧很流行，虽然针对ReLU的一些缺点，如没有负输出、零点处导数突变等，一些大牛做出了改进，如 LReLU、PReLU、ELU、SELU、GELU等，但实际使用中差距并不大，尤其是在使用BN 后。

除此之外，Ranacgabdran 等还根据自动搜索提出了一些激活函数，它们大都是基本函数的组合。其中效果比较好的如 swish。

# Sigmoid

