---
layout:     post
title:      "Tensorflow 笔记（六）"
subtitle:   "正则化项"
date:       2019-04-16 00:15:18
author:     "Pelhans"
header-img: "img/tensorflow.jpg"
header-mask: 0.3 
catalog:    true
tags:
    - Deep Learning
---

> 眼睛看过只是别人的，整理出来并反复学习才是自己的。

* TOC
{:toc}

# 概览

**正则化的本质就是对参数的先验假设**。通过对参数的正则化，以偏差的增加换取方差的减少，从而使得机器学习算法的泛化性增加。**偏差度量着偏离真实函数或者参数的误差期望，而方差度量着数据上任意特定采样可能导致的估计期望的偏差**。因此高偏差相当于模型欠拟合，而高方差是过拟合，导致泛化能力弱。大多数机器学习算法则是在偏差-方差，经验风险-结构风险之间做权衡。将正则项 $\Omega(\theta)$加入目标函数 J，有：

$$ \tilde{J}(\theta; X, y) = J(\theta; X, y) + \alpha\Omega(\theta) $$

其中 $\alpha\in[0, \infty]$ 是权衡范数惩罚项$\Omega$和标准目标函数 J 相对贡献的超参数，当 $\alpha$为0时相当于没有正则化。添加正则化项可以减小某些衡量标准下参数的规模。

一般而言，在神经网络中的正则化项只对权重 w 做，而不对偏置 b 做，精确拟合偏置所需的数据通常比拟合权重少得多。这是因为每个权重 w 会指定两个变量如何相互作用，而每个偏置只控制单一变量，这意味着我们不对其进行正则化也不会导致太大的方差。另外正则化偏置参数可能会导致明显的欠拟合。

下面介绍常用的正则化方法，包含 L1、L2正则化、Dropout、早停止、Bagging等集成方法、参数绑定和参数共享。

# L2 正则化

L2正则也被称作岭回归或 Tikhonov 正则。L2 正则化项公式为：

$$ \Omega(\theta) = \frac{1}{2}||w||_{2}^{2} $$

该**正则化项使得权重更加接近原点(实际上可以令他接近任意点，也有正则化效果)。具体来说，在显著减小目标函数方向上的参数会保留的相对完好。在无助于目标函数减小的方向(对应Hessian矩阵较小的特征值)上改变参数不会显著增加梯度。这种不重要方向对应的分量会在训练过程中因正则化而衰减掉。**

我们可以从两个层次来理解，首先从单步更新来说：

$$ \tilde{J}(w; X, y) = \frac{\alpha}{2}w^{T}w + J(w; X, y) $$

与之对应的梯度为：

$$ \nabla_{w}\tilde{J}(w; X, y) = \alpha w + \nabla_{w}J(w; X, y) $$

因此更新策略为：

$$ w \leftarrow (1-\epsilon\alpha)w - \epsilon\nabla_{w}J(w; X, y) $$

可以看到，加入正则项后，每步执行通常的梯度更新之前先收缩权重向量，以此控制权重大小。

另一方面，也可以从整个训练过程中来看这个问题。首先假设 $w^{*}$ 为为正则化的目标函数取得最小训练误差时的权重。令目标函数 J 在 $w^{*}$ 处展开并忽略二阶以上项，有：

$$ \hat{J}(\theta) = J(w^{*}) + \frac{1}{2}(w-w^{*})^{T}H(w-w^{*}) $$

其中一阶项的导数为0没有写。当$\hat{J}$取得最小值时，它的梯度等于0：

$$ \nabla_{w}\hat{J}(w) = H(w-w^{*}) $$

现在考虑加入正则项后，它的梯度也应该为零，因此：

$$ \alpha \tilde{w} + H(\tilde{w}-w^{*}) = 0 $$

$$ \tilde{w} = (H + \alpha I)^{-1}Hw^{*} $$

可以看出，当 $\alpha$ 趋近于0时，趋近于没正则项。对上式更进一步，因为H是半正定的，对其分解成对角矩阵$\Lambda$和标准正交基Q，并且有 $H = Q\Lambda Q^{T} $$，因此有：

$$
\begin{aligned}
\tilde{w} & = (Q\Lambda Q^{T} + \alpha I)^{-1}Q\Lambda Q^{T} w^{*} \\
 & = [Q(\Lambda + \alpha I)Q^{T}]^{-1}Q\Lambda Q^{T}w^{*} \\
 & = Q(\Lambda + \alpha I)^{-1}\Lambda Q^{T}w^{*}
\end{aligned}
$$

可以看到权重衰减的效果是沿着由 H 的特征向量所定义的轴缩放 $w^{*}$。具体来说，我们会根据 $\frac{\lambda_{i}}{\lambda_{i}+\alpha}$ 因子缩放与 H 第 i 个特征向量对齐的 $w^{*}$的分量。沿着 H 特征值较大的方向，正则化的影响较小，而 $\lambda_{i} << \alpha$ 的分量则会收缩到几乎为0.

![](/img/in-post/tensorflow/L2_shiyi.png)

还可以从参数的先验角度来看。现在我们假设n个参数服从正态分布 $w \sim N(0, \sigma^{2})$，那么它的负对数似然为：

$$ E_{w}(log g(w)) = -log(\prod_{i=1}^{n}\frac{1}{2\pi\sigma^{2}}e^{-\frac{w_{i}^{2}}{2\sigma^{2}}}) = \frac{1}{2}\log(2\pi \sigma^{2}) + \frac{1}{2\sigma^{2}}\sum_{i}w_{i}^{2} $$

忽略掉常数项，上式可以简化为 $\lambda|w|^{2}$，就得到了 L2 正则化的表达式。


