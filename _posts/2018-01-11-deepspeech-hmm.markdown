---
layout:     post
title:      "语音识别笔记(三)" 
subtitle:   "隐马尔科夫模型-HMM"
date:       2018-01-11 21:46:28
author:     "Pelhans"
header-img: "img/post_deepspeech_ch1_ch2.jpg"
header-mask: 0.3 
catalog:    true
tags:
    - ASR
    - NLP
---


> HMM作为一种典型的生成模型，在各个领域被广泛应用。我之前一直以为在NLP的序列标注任务中已经完全被CRF取代了，但最近的学习才发现它惊人的生命力。

* TOC
{:toc}

# 第三讲

个人由于多多少学过一些马尔科夫相关内容，因此关于过于基础的知识和公式推导此处将被略去，如有疑问，还请在评论区留言。

# 概述

概率图模型是在概率模型的基础上，使用了基于图的方法来表示概率分布，是一种通用化的不确定性知识和处理方法。下图给出常见的图模型：

![](/img/in-post/deepspeech_ch3/deepspeech_ch3_1.png)

有关图模型的知识读者可以查看机器学习与模式识别(PRML)一书的第八章。

# 马尔科夫模型

若系统在时间t的状态q只与其在时间t-1的状态相关，即：

$$P(q_{t} = s_{i} | q_{t-1}, q_{t-1} = s_{j}, q_{t-2} = s_{k}, \ldots) = P(q_{t} = s_{i} | q_{t-1} = s_{j})$$

那么系统构建成一个离散的一阶马尔科夫链(Markov Chain)。

若只考虑上式独立于时间t的随机过程:

$$P(q_{t} = s_{j} | q_{t-1} = s_{i}) = a_{ij}  , i,j \in [1, N]$$

则该过程为马尔科夫模型，其转移概率$$a_{ij}$$需满足半正定和归一的条件。

# 隐马尔科夫模型 

在在马尔科夫模型中，每个状态代表了一个可观察的事件，所以马尔科夫模型有时叫做可视马尔科夫模型。但在实际应用中我们有时不知道模型所经过的状态序列，只知道状态的概率函数，因此该模型是一个双重的随机过程。其中模型的转换状态是隐蔽的，可观察事件的随机过程是隐蔽的状态转换过程的随机函数。

下图给出HMM的图解：

![](/img/in-post/deepspeech_ch3/deepspeech_ch3_2.png)

### 举个例子

上面的定义看以来很晦涩，我们来举个例子来理解它：

假设在暗室中由N个口袋，每个口袋中由M种不同颜色的求。一个实验员根据某一概率分布随机的选取一个初始口袋，从中根据不同颜色的求的分布概率随机的取出一个球并记录下该球的颜色。而后再根据口袋的概率分布选取一个口袋，再根据不用管色球的概率分布随机选取一个球，记录下颜色。重复这个过程我们就得到了一串标记球颜色的序列，如“红黄红蓝...”。当你把这串序列给暗室外的人看的时候，他们只是看到最终球的颜色序列，但不知道口袋的序列。

在上面的例子中，口袋对应于HMM中的隐藏状态，而颜色序列则代表可观察的输出序列。从一个口袋转向另一个口袋代表状态间的转换，从口袋中取球代表该状态的观察状态输出。

## HMM的组成部分

从上面的例子可以看出，HMM由如下几个部分组成。

1) 模型中状态的数目N(上例中口袋的数目)。

2) 从每个状态可能输出的不同符号的数目M（上例中口袋的数目球的不同颜色的数目)；

3) 状态转移概率矩阵$$ A = {a_{ij}}$$ ,$$a_{ij}$$为实验员从一个口袋$$s_{i}$$转向另一个口袋$$s_{j}$$取球的概率，其中:

$$a_{ij} = P(q_{t} = s_{j} | q_{t-1} = s_{i}), ~1 \leq i, j\leq N$$

$$ a_{ij} \geq 0$$

$$\sum\limits_{j=1}^{N} a_{ij} = 1$$

4)从状态$$s_{j}$$观察到符号$$v_{k}$$的概率分布矩阵$$B = {b_{j}(k)}$$, $$b_{j}(k)$$为实验员从第j个口袋中抽取出第k种颜色的球的概率，其中：

$$ b_{j}(k) = P(O_{i} = v_{k} | q_{t} = s_{i}), ~1\leq j \leq N;1 \leq k \leq M$$

$$b_{j}(k) \geq 0$$

$$\sum\limits_{k=1}^{M}b_{j}(k) = 1$$

观察符号的概率又称为符号发射概率。

5) 初始状态概率分布$$\pi = {\pi_{i}}$$，其中:

$$ \pi_{i} = P(q_{i} = s_{i}), ~1\leq i \leq N$$

$$\pi_{i} \geq 0$$

$$\sum\limits_{i=1}^{N}\pi_{i}=1$$

一般地，一个HMM记为一个五元组$$\mu = (S, K, A, B, \pi)$$,其中S为状态的集合，K为输出符号的集合，$$\pi, A, B$$分别为初始状态的概率分布、状态转移概率和符号发射概率。当考虑潜在时间随机地生成表面事件时，HMM是非常有用的。

## HMM的三个基本问题

1) 计算likelihod：给定HMM的模型参数和一个观察序列，计算出观察序列O的概率分布矩阵
$$P(O | \mu)$$,可采用前向算法进行解决。

2) 解码问题：给定一个HMM的模型参数$$\mu$$和观察序列$$O = O_{1}O_{2}\ldots O_{T}$$，找到最优的状态序列$$Q = q_{1}q_{2}\ldots q_{T}$$。可以用Viterbi算法解决。

3) HMM的训练： 给定一个观察序列$$O = O_{1}O_{2}\ldots O_{T}$$和HMM一系列可能的状态，如何调节HMM的参数使得
$$P(O | \mu)$$最大。

以下将对应于三个问题分别进行解决。

### 计算likelihood: 前向算法

对于该问题，我们采用前向算法来进行解决。

首先我们定义一个前向变量$$\alpha_{t}(i)$$：

$$\alpha_{t}(i) = P(O_{1}O_{2}\ldots O_{t}, q_{t} = s_{i} | \mu)$$

它的含义为在时间t,HMM输出了序列O_{1}O_{2}\ldots O_{t}，并且位于状态$$s_{i}$$的概率。

前向算法的主要思想是如何快速地计算前向变量$$\alpha_{t}(i)$$，这样我们就可以根据$$\alpha_{t}(i)$$计算出
$$P(O | \mu)$$,即:

$$P(O | \mu) = \sum\limits_{s_{i}}P(O = O_{1}O_{2}\ldots O_{T}, q_{T} = s_{i} | \mu) = \sum_{i=1}^{N}\alpha_{T}(i)$$

经过观察可知，在时间t+1的前向变量可以根据在时间t时的前向变量$$\alpha_{t}(1), \alpha_{t}(2),\ldots,\alpha_{t}(N)$$的值采用动态规划算法来归纳计算:

$$\alpha_{t+1}(j) = (\sum_limits_{i=1}^{N}\alpha_{t}(i)a_{ij})b_{j}(O_{t=1})$$

公式的结构可以分解来进行解释，首先根据前向变量的定义, 在时间t时到达状态$$s_{i}$$并输出观察序列$$O = O_{1}O_{2}\ldots O_{t}$$时的前向变量为$$\alpha_{t}(i)$$。从状态$$s_{i}$$转移到状态$$s_{j}$$并在状态输出$$O_{t=1}$$的概率为$$a_{ij}\times b_{j}(O_{t+1})$$。因此从初始时间到t+1整个过程的概率为:$$\alpha_{t}(i)\times\alpha_{ij}\times b_{j}(O_{t+1})$$。由于HMM可以从不同的$$s_{i}$$转移到$$s_{j}$$，一共有N个不同的状态，因此就得到了上式。

前向算法总结为:

步骤1: 初始化:

$$\alpha_{1}(i) = \pi_{i}b_{i}(O_{1}),~1\leq i \leq N$$

步骤2: 归纳计算:

$$\alpha_{t+1}(j) = (\sum\limits_{i=1}^{N}\alpha_{t}(i)a_{ij})b_{j}(O_{t=1}), ~1\leq t\leq T-1$$

步骤3：求和终结

$$P(O | \mu ) = \sum\limits_{i=1}^{N}\alpha_{T}(i)$$

相对应的还有后向算法，后向变量的定义为: 

$$\beta_{t}(i) = P(O_{t=1}O_{t+2}\ldots O_{T} | q_{t} = s_{i}, \mu)$$

这样我们就可以类比于前向算法对后向变量进行更新了。

### 解码问题：Viterbi算法

维特比算法用来求解HMM的第二个问题，即给定一个HMM的模型参数$$\mu$$和观察序列$$O = O_{1}O_{2}\ldots O_{T}$$，找到最优的状态序列$$Q = q_{1}q_{2}\ldots q_{T}$$.因此我们的目标是在给定模型参数$$\mu$$和观察序列O的条件下，使得条件概率
$$P(Q | O,\mu)$$最大的状态序列，即:

$$Q_{*} = \arg\max\limits_{Q}P(Q | O, \mu)$$

此时优化的是整个状态序列。为了应用维特比算法，我们首先定义一个维特比变量$$\delta_{t}(i)$$:

$$\delta_{t}(i) = \max\limits_{q_{1},q_{2},\ldots q_{t-1}} P(q_{1}, q_{2}, \ldots, q_{t} = s_{i}, O_{1}O_{2}\ldots O_{t} | \mu)$$

与前向变量类似，$$\delta_{t}(i)$$有如下递归关系:

$$\delta_{t+1}(i) = \max\limits_{j}[\delta_{t}(j)\cdot a_{ij}]\cdot a_{i}(O_{t+1})$$

这种递归关系能够使我们使用动态规划搜索技术。同时为了记录在时间t时，HMM通过哪一条概率最大的路径到达状态$$s_{i}$$，维特比算法设置了另外一个变量$$\psi_{t}(i)$$用于路径记忆，让$$\psi_{t}(i)$$记录该路径上状态$$s_{i}$$的前一个状态。综合上面所述，我们得到完整的维特比算法:

1): 初始化:

$$\delta_{1}(i) = \pi_{i}b_{i}(O_{1}), ~1\leq i \leq N$$

$$\psi_{1}(i) = 0$$

2): 归纳计算:

$$\delta_{t}(j) = \max\limits_{1\leq i\leq N}[\delta_{t-1}\cdot a_{ij}]\cdot b_{j}(O_{t}), ~2\leq t \leq T;~1\leq j \leq N$$

记忆回退路径:

$$\psi_{t}(j) = \arg\max\limits_{1\leq i\leq N}[\delta_{t-1}\cdot a_{ij}]\cdot b_{j}(O_{t}), ~2\leq t \leq T;~1\leq j \leq N$$

3) 终结:

$$Q_{T*} = \arg\max\limits_{1\leq i\leq N}[\delta_{T}(i)]$$

$$P_{T*} = \max\limits_{1\leq i\leq N}[\delta_{T}(i)]$$

4)路径(状态序列)回溯:

$$q_{*} = \psi_{t+1}(q_{t+1,*}),~~t = T-1, T-2,\cdots,1$$

维特比算法的时间复杂度和前向算法,后向算法一样,都是$$O(N^{2}T)$$.在实际应用中,往往会所搜n个最佳路径.

### HMM的训练:前向后向算法

前向后向算法用来求解HMM中的第三个问题,即给定一个观察序列$$O = O_{1}O_{2}\cdots O_{T}$$,如何调节模型参数,
使得$$P(O, | \mu)$$最大化.

期望最大化(Exoectation maximization, EM)算法可以用于含有隐变量的统计模型的参数最大似然估计.它的核心思想是初始时随机地给模型的参数赋值,该赋值遵循模型对参数的限制.而后我们将得到模型的参数$$\mu_{0}$$.根据$$\mu_{0}$$求得模型中隐变量的期望值.由该期望值又可以重新得到模型参数的新估计值,由此得到模型的新参数$$\mu_{1}$$.循环进行该过程,知道参数收敛于最大似然估计值.

这里我们介绍EM算法的一种具体实现方法,称之为Baum-Welch算法或前向后向算法.

从上面的描述中,我们看到隐状态的期望和参数的更新公式是我们关心的,因此我们先求期望.

给定HMM的参数$$\mu$$和观察序列$$O = O_{1}O_{2}\cdots O_{T}$$,在时间t位于状态$$s_{i}$$,时间t+1位于状态$$s_{j}$$的概率
$$\xi_{t}(i, j) = P(q_{t} = s_{i}, q_{t+1} = s_{j} | O, \mu)(1\leq t\leq T, 1\leq i,j\leq N)$$可以由下面的公式计算获得:

$$\xi_{t}(i, j) = \frac{P(q_{t} = s_{i}, q_{t+1} = s_{j}, O \mu)}{P(O | \mu)}$$

$$= \frac{\alpha_{t}(i)a_{ij}b_{j}(O_{t+1})\beta_{t+1}(j)}{P(O | \mu)}$$

$$= \frac{\alpha_{t}(i)a_{ij}b_{j}(O_{t+1})\beta_{t+1}(j)}{\sum\limits_{i=1}^{N} \sum\limits_{j=1}^{N} \alpha_{t}(i)a_{ij}b_{j}(O_{t+1})\beta_{t+1}(j)}$$

给定HMM的参数和观察序列,在时间t位于状态$$s_{i}$$的概率$$\gamma_{t}(i)$$为:

$$\gamma_{t}(i) = \sum\limits_{j=1}^{N}\xi_{t}(i, j)$$

它表示对下一所有可能状态j的求和.得到期望后,我们就可以对参数进行更新了.

$$average(\pi_{i}) = P_(q_{1} = s_{i} | O,\mu) = \gamma_{1}(i)  $$

$$average(a_{ij}) = \frac{\sum\limits_{t=1}^{T-1}\xi_{t}(i,j)}{\sum\limits_{t=1}^{T-1}\gamma_{t}(i,j)}$$

$$average(b_{j}) = \frac{\sum\limits_{t=1}^{T}\gamma_{t}(j)\times\delta(O_{t},v_{k})}{\sum\limits_{t=1}^{T}\gamma_{t}(j)}$$

有了上述算法,我们就可以按照E-M的步骤进行计算.E步骤计算期望值$$\xi_{t}(i,j), \gamma_{t}(i)$$.M步骤用E步骤得到的期望值重新估计参数,得到新的模型参数.重复这些步骤直至模型参数收敛.

# 絮叨

关于马尔科夫真的有太多需要写的,也有太多需要记忆的,毕竟在深度学习席卷ML之前,HMM是霸主一样的存在,被应用于各行各业,这里只是给出很基础的一些概念.其他很多都是这个的衍生.我自己之前虽然多次学习,但一直都是模模糊糊,每次看完后都会忘掉.直到这次完整写出来理出思路.愿随着笔记的增长,我能对ML和NLP理解的更深刻,走的更远.GOod Luck~
