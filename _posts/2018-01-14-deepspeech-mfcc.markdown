---
layout:     post
title:      "语音识别笔记 (五)" 
subtitle:   "MFCC,搜索-解码,Embedded Training"
date:       2018-01-14 20:39:28
author:     "Pelhans"
header-img: "img/post_deepspeech_ch1_ch2.jpg"
header-mask: 0.3 
catalog:    true
tags:
    - ASR
    - NLP
---


> 讲完声学模型的建立,现在我们就来讲讲它的输入和输出应该怎么处理.

* TOC
{:toc}

# 第五讲

## 梅尔频率倒谱系数(MFCC)

在一个ASR系统中,第一步要做的就是特征提取.从前面的计算音系学部分可知,声音的音高等信息能体现一个人的语音特性.一个人的语音特性可以体现在声道的形状上,如果可以准确知道这个形状,那么我们就可以对产生的音素进行准确的描述.

声道的形状在语音短时功率谱的包络中显示出来.而MFCCs就是一种准确描述这个包络的一种特征.下面讲解MFCC的获取流程.

### MFCC获取流程

1) 首先我们有一个声谱,它长成下面这个样子.它的横轴代表时间,纵轴代表振幅.很常见.

![](/img/in-post/deepspeech_ch5/deepspeech_ch5_1.jpg)

2) 我们知道ASR系统的实现是基于帧的,每帧大概10ms或者20ms.这样声音就变成了一个一个离散的片段.因为防止分帧时信息的丢失,因此帧与帧之间是有交叉的.

3)有了帧后,我们对其进行短时快速傅里叶变换(短时FFT)就得到了频谱.频谱表示频率与能量的关系.在实际应用中,频谱大体分为线性振幅谱,对数振幅谱和自功率谱.对于对数振幅谱由于振幅进行了对数计算,因此从坐标为分贝.

![](/img/in-post/deepspeech_ch5/deepspeech_ch5_2.jpg)

有时为了将时间信息加进来,我们把这些幅度映射到一个灰度级表示,幅度越大则颜色越深.如下图所示:

![](/img/in-post/deepspeech_ch5/deepspeech_ch5_3.jpg)

此时的声谱就很有用啦~首先通过声谱我们可以观察出音素的属性,另外通过共振幅和它的转变可以很好的识别声音.同事还可以直观的评估TTS系统.

4) 准备提取包络啦,上面我们说到在语音的频谱图中,峰值表示语音的主要频率成分(共振峰),它携带了声音的辨识属性.同时我们也要获得峰值的变化情况,总结起来我们就是要获取这个频谱的包络...

下图给出一个常见的频谱图,其中红色部分就是我们想要的包络.

![](/img/in-post/deepspeech_ch5/deepspeech_ch5_4.jpg)

为了获取它我们先对频谱做FFT,由于是在频谱上做,所以相当于逆傅里叶变换(IFFT),需要注意的是我们是在频谱的对数域上处理的,此时在对数频谱上做IFFT就相当于在一个伪频率坐标轴上描述信号.

下图给出结果,可以看到,包络主要由在伪频率坐标轴上的低频部分描述,而细节部分则主要由高频部分描述.因此我们将频谱$$log x[K]$$处理得到x[k]加一个低通滤波器就可以得到包络了.

上述六成总结如下图所示:

![](/img/in-post/deepspeech_ch5/deepspeech_ch5_5.jpg)

5) 我们上面获得了倒谱,那什么是梅尔倒谱呢?

想了解它就要首先说一说梅尔频率,实验发现,人耳就像一个滤波器一样,在低频部分(1000Hz以下)性能超级好,但高频部分感觉就没那么灵敏了.因此人耳是一个非线性系统,梅尔频率就是基于该想法,它将不统一的频率转回统一的频率,在梅尔频域内,人对音调的感知度为线性关系.

将普通频率转化为梅尔频率的公式为:

$$ mel(f) = 2565 \times log_{10}(1+f/700)$$

用图表示为:

![](/img/in-post/deepspeech_ch5/deepspeech_ch5_6.jpg)

梅尔频谱上面获得的倒谱系数就成为梅尔频率倒谱系数,简称MFCC.

### 小结
    1. 先对语音进行预加重、分帧和加窗；（加强语音信号性能（信噪比，处理精度等）的一些预处理）
    2. 对每一个短时分析窗，通过FFT得到对应的频谱；（获得分布在时间轴上不同时间窗内的频谱）
    3. 将上面的频谱通过Mel滤波器组得到Mel频谱；（通过Mel频谱，将线形的自然频谱转换为体现人类听觉特性的Mel频谱）
    4.在Mel频谱上面进行倒谱分析（取对数，做逆变换，实际逆变换一般是通过DCT离散余弦变换来实现，取DCT后的第2个到第13个系数作为MFCC系数），获得Mel频率倒谱系数MFCC，这个MFCC就是这帧语音的特征；（倒谱分析，获得MFCC作为语音特征）

这样就可以通过这些倒谱向量对语音分类器进行训练和识别了。

## 解码

解码就是在此处指的是给定声学观察量的情况下,我们怎样选取最高先验概率的文本.这里我们采用维特比算法对其进行解码.任务就选用数字识别任务,因为它比较简单,词汇量也小.

首先我们明确一下HMM模型的基本组分:

$$Q = q_{1}q_{2}\ldots q_{N}~~$$ 表示一系列子音素状态.

$$A = a_{01}a_{02}\ldots a_{n1}\ldots a_{nm}~~$$ 表示转移矩阵A,一个状态可以转移到下一个状态也可以转回自身.

$$B= b_{i}(o_{t})~~$$ 可观察量的似然值,也叫发射概率:由子音素态i产生倒谱特征向量$$o_{t}$$的概率.

明确以上变量后,我们需要知道HMM的结构,对于数字识别任务来说它可以来自于词典和发音词典.这样对于每个数字的HMM结构,我们讲发音词典中的音素分为三个子音素,并把他们连接起来.为了更符合实际,我们还在每个音素之间加上静音状态.将他们连在一起就形成如下图所示的状态.

![](/img/in-post/deepspeech_ch5/deepspeech_ch5_7.jpg)

好了,准备工作结束,可以用维特比算法了.

每维特比算子$$v_{t}(j)$$表示在HMM的状态j时,它通过了最可能的状态序列$$q_{1}q_{2}\ldots q_{t-1}$$并生成了可观察状态$$o_{1}o_{2}\ldots o_{t}$$的概率.公式表达为:

$$v_{t}(j) = \max\limits_{i=1}^{N}v_{t-1}(i)a_{ij}b_{j}(o_{t})$$

其中:

$$v_{t-1}(i)~~$$ 是时间t-1的维特比算子;

$$a_{ij}~~$$是状态$$q_{i}$$转移到$$q_{j}$$的概率;

$$b_{j}(o_{t})~~$$表示发射概率;

给定了以上状态后,我们就可以使用前面介绍的维特比算法进行计算了.下面给出维特比算法的流程以便回忆.

![](/img/in-post/deepspeech_ch5/deepspeech_ch5_8.jpg)

好了,上面对数字识别任务的解码做了简要概述,但实际应用中可不是每个词的开始和结束都那么明显,往往都是一个词直接连着下一个词,对于这种情况我们需建立对应的HMM结构,下图给出一个2-gram的HMM结构.

![](/img/in-post/deepspeech_ch5/deepspeech_ch5_9.jpg)

此时一个词的结尾子音素直接转移到下一个词的开始子音素,当然这种转移我们可以借助$$P(W)$$来帮助我们解决.下图给出此时的维特比格点图:

![](/img/in-post/deepspeech_ch5/deepspeech_ch5_10.jpg)

## 嵌入式训练

在之前的部分,我们完成了声学特征向量提取,声学模型的建立,HMM的解码等部分,现在我们来介绍训练部分.

最理想的情况是使用手动标记的标签数据集,其中每个声音中的词都被手动分开标记,并且其中的音素也都被很好的标记.我们只需要简单的对其进行统计学习得到转移矩阵A和发射概率B即可.但现实是人工对每帧进行标记工作量太大了甚至不可能完成,因此语音标记系统会在整个句子上对每个音素进行嵌入式训练,它将在训练步骤中自动完成词汇切分,音素对齐的工作.下面以简单数字语音识别系统来作为例子介绍.

数字识别任务中的每个波形文件包含一串数字,同事该数据集还提供了发音词典和音素集等同来定义HMM的状态(未训练).这样,对于每个句子,我们就可以建立一系列的HMM状态了,如下图所示.

![](/img/in-post/deepspeech_ch5/deepspeech_ch5_11.jpg)

现在我们可以使用Baum-Welch算法来对其进行训练得到转移矩阵A和发射概率B.其中A的初始化可以设置为0.5或0.B的初始化概率可以通过设置高斯函数的均值和方差等于全体训练数据集的均值和方差.

除了上面的Baum-Welch算法外,我们还有另一个更高效的训练方法-Viterbi 训练.它不是采用BW算法的那种累加所有可能路径的方式,而是只计算最优的概率路径.这种在训练数据集上跑Viterbi算法的方法叫维特比对齐或强制对齐(forced alignment),

在维特比训练中,我们知道哪个词对应的观察量,所以我们可以通过设定合适的转移概率强制使维特比算法通过确定的词.因此强制维特比算法就被简化成了通常的维特比解码问题,即只需要指出正确的音素状态序列而不需要考虑词序列.因此强制对齐的结果就是一条对应于训练可观察序列的最佳路径.我们就可以用这种对齐 的HMM状态到可观察态上来重新计算HMM的参数.重复这个过程知道HMM的参数收敛即可.

## 絮叨

基于GMM-HMM的语音识别框架算是大体写完了,但脑海中还是有很多模糊的地方,因此上面有可能存在一些理解不到位写错的地方,如有发现还请指正.接下来还剩一个语音识别的高级话题了,写完那部分就是大体写一下基于深度学习的语音识别框架.也就是还剩下3讲左右.再然后就是上Kaldi实战了...到时候理解更深了就回来再审阅一下上面的内容....
