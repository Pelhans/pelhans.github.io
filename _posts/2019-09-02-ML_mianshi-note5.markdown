---
layout:     post
title:      "隐马尔可夫模型 HMM"
subtitle:   ""
date:       2019-09-02 00:15:18
author:     "Pelhans"
header-img: "img/kg_bg.jpg"
header-mask: 0.3 
catalog:    true
tags:
    - PRML
---


* TOC
{:toc}

# 隐马尔可夫模型

## HMM 的基本概念和定义

HMM 是关于时序的概率模型, 描述一个隐藏的马尔科夫链随机生成不可观测的状态随机序列, 再由各个状态生成一个观测而chanshengtree观测随机序列的过程. HMM 的马尔科夫链随机生成的状态的序列称为状态序列. **每个状态生成一个观测**, 而由此产生的观测的随机序列, 称为观测序列. 序列的每一个位置又可以看作是一个时刻.

从潜在变量的角度来说, 我们可以使用潜在变量的马尔科夫链来表示顺序数据, 每个观测都以对应的潜在变量的状态为条件. 对于顺序数据来说, 下面的图描述了两个模型. 如果潜在变量是离散的, 那么我们就得到了HMM(HMM 的观测变量是离散的还是连续的都可以, 并且可以使用许多不同的条件概率分布建模). 如果潜在变量和观测变量都是高斯变量(结点的条件概率分布对于父节点的依赖是线性高斯的情形), 那么我们就得到了线性动态系统.

举个例子, 对于词性标注任务来说, 观测序列就是我们的词. 状态序列就是词对应的词性标签.

[](img/in-post/deepspeech_ch3/deepspeech_ch3_2.png)

HMM 由初始条件概率分布, 状态转移概率分布以及观测概率分布确定. 

设 $Q\in \{q_{1}, q_{2},\dots, q_{N} \} 是所有可能的状态的集合, $V\in \{v_{1}, v_{2},\dots, v_{M} \}. 是所有可能的观测的集合. 

I 是长度为 T 的状态序列, O 是对应的观测序列. $I = (i_{1}, i_{2}, \dots, i_{T})$, $O_{o_{1}, o_{2}, \dots, o_{T}}$.

A 是状态转移概率矩阵: $A = [A_{ij}]_{N\times N} $. 其中 $a_{ij} = P(i_{t+1}=q | i_{t}=q_{i}), ~~~ i=1, 2, \dots, N; j=1,2,\dots, N$. 表示 在t时刻处于状态 $q_{i}$ 的条件下,在时刻 t+1 转移到状态 $q_{j}$ 的概率.

B 是发射概率矩阵. $B = [b_{j}(k)]_{N\times N}$, 其中 $b_{j}(k) = P(o_{t} = v_{k}|i_{t} = q_{j}), k=1,2,\dots, M; j = 1, 2,\dots,N$. 表示在时刻 t 处于状态 $q_{j}$ 的条件下生成观测 $v_{k}$ 的概率.

$\pi$ 是初始概率向量, $\pi = (\pi_{i})$, 其中 $\pi_{i} = P(i_{1}=q_{i}), i=1,2,\dots, N$. 表示 t=1 处于状态 $q_{i}$ 的概率.

HMM 模型由上面三个参数所决定. 因此HMM 的模型参数为 $\theta = (A, B, \pi)$

从定义可知, HMM 做了两个基本假设:

* 其次马尔可夫性假设: 假设隐藏的马尔科夫链在任意时刻 t 的状态只依赖于其前一时刻的状态, 与其他时刻的状态以及观测无关, 也与时刻 t 无关.    
* 观测独立性假设: 假设任意时刻的观测只依赖于该时刻的马尔科夫链的状态, 与其他观测及状态无关.

如果我们放松第二个假设,就得到最大熵马尔科夫模型(MEMM). MEMM 描述的是在当前观察值 $o_{t}$ 和前一个状态 $i_{t-1}$ 的条件下, 当前状态 $i_{t}$ 的概率. 从图结构来说, MEMM 描述了由观察变量生成隐藏状态序列的模型.

从生成式的观点看 HMM. 首先我们选择初始的潜在变量 $z_{1}$, 概率由参数 $\pi_{k}$ 来控制, 染后采样对应的观测 $x_{1}$. 采样过程由发射概率控制. 现在我们使用已经初始化的 $z_{1}$ 的值, 根据转移概率来选择 $z_{2}$ 的状态. 继续对 $z_{2}$ 采样, 生成 $x_{2}$. 以此类推, 得到数据.

# HMM 的基本问题

HMM 有三个基本问题:

* 似然性计算: 给定HMM的模型参数 $\lambda$ 和一个观察序列 O ，计算出观察序列 O 的概率分布矩阵 
$P(O|\lambda)$. 可以用 前向算法, 后向算法, 前后向算法解决.    
* 学习问题: 已知观测序列 O, 估计模型参数 $\lambda$, 使得似然函数 $ P(O|\lambda)$ 最大. 可以用极大似然估计的方法估计参数, 如 EM 算法.    
* 解码问题: 给定 HMM 的模型参数和 观测序列O. 找到最优的状态序列 Q. 可以用 维特比算法解决.

## 似然性计算

直接计算在考虑整个序列T全局的情况下, 时间复杂度为 $O(T\times Q^{T})$. 计算方式是每个时间步上都有 Q 个可能的状态, 整个序列上就是 $Q^{T}$. 复杂度爆炸, 因此需要前后向这种局部算法.

### 前向算法

前向算法是基于状态序列的路径结构递推计算
$P(O|\lambda)$. 给定隐马尔可夫模型参数 $\lambda$, 定义前向算子 
$\alpha_{t}(i) = P(o_{1}, o_{2}, \dots. o_{t}. i_{t}=i|\lambda)$. 它表示在时刻 t 时的观测序列为 $o_{1}, o_{2}, \dots, o_{t}$, 且时刻 t 时状态为 $q_{i}$ 的概率.

根据定义, 我们可以写出递推公式:

$$ \alpha_{t+1}(i) = [\sum_{j=1}^{Q}\alpha_{t}(j)a_{j,i}]b_{i}(o_{t+1}) $$

其中 中括号内表示, 再时刻 t 的观测序列为 $o_{1}, o_{2}, \dots, o_{t}$, 并且在 t+1 时刻处于状态 $q_{i}$ 的概率. 后面再乘以 t+1 时刻状态 i 的发射概率, 就得了 t+1 时刻的前向算子. 从图上来看就是下图这样.

![](/img/in-post/ml_mianshi/forward_HMM_prob.png)

有了递推关系, 我们可以写出前向算法流程:

* 输入为: 模型参数 $\lambda$, 观测序列 O. 输出为观测序列概率
$P(O|\lambda)$    
* 计算初始值, $\alpha_{1}(i) = \pi_{i}b_{i}(o_{1}), ~~~i=1,2,\dots, Q$    
* 利用递推公式递推计算前向算子.    
* 在时刻 T 终止, 即 
$P(O|\lambda) = \sum_{i=1}^{Q}\alpha_{T}(i)$.

可以看到, 其算法高效的原因是可以局部计算前项概率, 而后利用路径结构将前向概率"递推"到全局, 算法复杂度为 $O(TQ^{2})$.

类似地, 我们可以定义后向算法.

## 后向算法
